# Pylogue

> A lightweight, dependency-injection-based chat framework for building LLM applications with FastHTML and WebSockets

## Overview

Pylogue is a modular Python framework for creating real-time chat applications with streaming LLM support. Built on FastHTML and WebSockets, it provides clean separation of concerns through dependency injection.

## Core Concepts

### 1. Architecture Layers

**Session Layer** (`pylogue.session`)
- Manages conversation state and message history
- `SessionManager`: Protocol for session storage (in-memory, Redis, DB)
- `ChatSession`: Individual conversation context with message list
- `Message`: Single message with role (User/Assistant) and content

**Service Layer** (`pylogue.service`)
- Business logic for message processing
- `ChatService`: Orchestrates responder calls with context
- `Responder`: Protocol for AI/LLM integrations
- Supports both streaming (async generators) and non-streaming responders

**Renderer Layer** (`pylogue.renderer`)
- UI component generation
- `ChatRenderer`: Renders messages, input forms, spinners
- `ChatCard`: Customizable message card styling with WCAG-compliant text colors

**Application Layer** (`pylogue.chatapp`)
- Full FastHTML + WebSocket integration
- `ChatApp`: Main application orchestrator
- `ChatAppConfig`: Configuration for styling, WebSocket, extensions

### 2. Responder Types

**Non-Streaming Responder**
```python
async def simple_responder(message: str, context=None) -> str:
    return f"You said: {message}"
```

**Streaming Responder (async generator)**
```python
class StreamingResponder:
    async def __call__(self, message: str, context=None):
        for token in generate_tokens(message):
            yield token
            await asyncio.sleep(0.05)  # Control streaming speed
```

The framework automatically detects async generators and streams tokens progressively to the UI.

## Quick Start

### Minimal Example (5 lines)

```python
from pylogue.chatapp import create_default_chat_app

async def my_responder(msg: str, context=None) -> str:
    return f"Echo: {msg}"

app = create_default_chat_app(responder=my_responder)
app.run(port=5001)
```

### Streaming LLM Example

```python
from pylogue.chatapp import create_default_chat_app

class MyStreamingLLM:
    async def __call__(self, message: str, context=None):
        # Yield tokens as they arrive from your LLM
        async for token in llm_stream(message):
            yield token

app = create_default_chat_app(responder=MyStreamingLLM())
app.run(port=5001)
```

### Custom Styling

```python
from pylogue.cards import ChatCard
from pylogue.chatapp import ChatAppConfig

card = ChatCard(
    user_color="#1a3a1a",
    assistant_color="#1a1a3a",
    user_emoji="ðŸ‘¤",
    assistant_emoji="ðŸ¤–",
    width="70%",
    font_size="1.2em"
)

config = ChatAppConfig(
    app_title="My AI Assistant",
    bg_color="#0a0a0a",
    markdown_enabled=True,
    syntax_highlighting=True,
    highlight_langs=["python", "javascript"]
)

app = create_default_chat_app(
    responder=my_responder,
    card=card,
    config=config
)
```

## Full Dependency Injection

For complete control, manually compose all layers:

```python
from pylogue.chatapp import ChatApp, ChatAppConfig
from pylogue.session import InMemorySessionManager
from pylogue.service import ChatService
from pylogue.renderer import ChatRenderer
from pylogue.cards import ChatCard

# 1. Session Management
session_manager = InMemorySessionManager()
# Or implement custom: RedisSessionManager, DatabaseSessionManager

# 2. Service Layer
chat_service = ChatService(
    responder=my_responder,
    context_provider=lambda session: session.get_messages()[-5:],  # Last 5 messages
    error_handler=custom_error_handler
)

# 3. Renderer
renderer = ChatRenderer(
    card=ChatCard(user_color="#2a2a2a"),
    ws_endpoint="/ws"
)

# 4. Config
config = ChatAppConfig(
    app_title="Custom App",
    initial_messages_factory=lambda: [
        Message(role="Assistant", content="Hello! How can I help?")
    ]
)

# 5. Compose
app = ChatApp(
    session_manager=session_manager,
    chat_service=chat_service,
    renderer=renderer,
    config=config
)

app.run(port=5001)
```

## Streaming Support

Pylogue automatically detects and handles streaming responders:

### How It Works

1. **Detection**: `ChatService` checks if responder is an async generator
2. **Streaming**: Yields tokens progressively via WebSocket
3. **UI Updates**: Each token triggers incremental message update
4. **No Buffering**: Tokens display as they arrive

### Example: Pydantic AI Streaming

```python
from pydantic_ai import Agent

class PydanticAIStreamer:
    def __init__(self, agent):
        self.agent = agent
        self.message_history = None
    
    async def __call__(self, text: str, context=None):
        async with self.agent.run_stream(
            text,
            message_history=self.message_history
        ) as response:
            async for token in response.stream_text(delta=True):
                yield token
            self.message_history = response.all_messages()

agent = Agent("openai:gpt-4", system_prompt="You are helpful.")
app = create_default_chat_app(responder=PydanticAIStreamer(agent))
```

### Example: OpenAI Streaming

```python
from openai import AsyncOpenAI

class OpenAIStreamer:
    def __init__(self, api_key, model="gpt-4"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
    
    async def __call__(self, message: str, context=None):
        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": message}],
            stream=True
        )
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

app = create_default_chat_app(responder=OpenAIStreamer(api_key="sk-..."))
```

## Context Management

Provide conversation context to your responder:

```python
def context_provider(session: ChatSession):
    """Extract last 10 messages as context"""
    messages = session.get_messages()
    return messages[-10:] if len(messages) > 10 else messages

chat_service = ChatService(
    responder=my_responder,
    context_provider=context_provider
)
```

Your responder receives context:

```python
class ContextAwareResponder:
    async def __call__(self, message: str, context=None) -> str:
        if context and isinstance(context, list):
            history_count = len(context)
            return f"Based on our {history_count} messages, here's my response..."
        return "Hello! Let's start our conversation."
```

## Error Handling

Custom error handlers for logging, notifications, or graceful degradation:

```python
from pylogue.service import ErrorHandler

class SlackErrorHandler:
    def __init__(self, webhook_url):
        self.webhook = webhook_url
    
    def __call__(self, error: Exception, message: str) -> str:
        # Send alert to Slack
        requests.post(self.webhook, json={
            "text": f"âŒ Error: {error}\nMessage: {message}"
        })
        return "I encountered an error. The team has been notified."

chat_service = ChatService(
    responder=my_responder,
    error_handler=SlackErrorHandler(webhook_url="https://...")
)
```

## Custom Session Storage

Implement `SessionManager` protocol for persistence:

```python
from pylogue.session import SessionManager, ChatSession, Message

class RedisSessionManager(SessionManager):
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def create_session(self, session_id: str, initial_messages=None) -> ChatSession:
        session = ChatSession(session_id, initial_messages or [])
        self.redis.set(f"session:{session_id}", session.to_json())
        return session
    
    def get_session(self, session_id: str) -> ChatSession:
        data = self.redis.get(f"session:{session_id}")
        return ChatSession.from_json(data) if data else None
    
    def delete_session(self, session_id: str):
        self.redis.delete(f"session:{session_id}")

# Use in app
app = ChatApp(
    session_manager=RedisSessionManager(redis_client),
    # ... other components
)
```

## Configuration Options

### ChatAppConfig

```python
ChatAppConfig(
    app_title="Chat Application",
    page_title="Chat",
    bg_color="#1a1a1a",
    header_style="text-align: center; padding: 1em; color: white;",
    ws_endpoint="/ws",
    markdown_enabled=True,
    syntax_highlighting=True,
    highlight_langs=["python", "javascript", "html", "css"],
    spinner_css="...",  # Custom loading spinner
    initial_messages_factory=lambda: [...]  # Initial conversation
)
```

### ChatCard

```python
ChatCard(
    user_color="#DCF8C6",        # User message background
    assistant_color="#E6E6E6",   # Assistant message background
    user_emoji="ðŸ—£ï¸",
    assistant_emoji="ðŸ•µï¸â€â™‚ï¸",
    width="60%",
    font_size="1.5em",
    padding="1.25em",
    border_radius="1em",
    margin="1em auto",
    box_shadow="0 2px 10px rgba(0,0,0,0.1)",
    # Text color auto-adjusts based on background (WCAG compliant)
)
```

## Testing

Each component is independently testable:

```python
# Test Session
from pylogue.session import ChatSession

session = ChatSession("test-id")
session.add_message("User", "Hello")
assert len(session) == 1
assert session.get_messages()[0].content == "Hello"

# Test Service
from pylogue.service import ChatService

async def test_responder(msg: str, context=None) -> str:
    return f"Test: {msg}"

service = ChatService(responder=test_responder)
response = await service.process_message("Hi")
assert "Hi" in response

# Test Renderer
from pylogue.renderer import ChatRenderer
from pylogue.session import Message

renderer = ChatRenderer()
messages = [Message(role="User", content="Test")]
html = renderer.render_messages(messages)
assert "Test" in str(html)
```

## Examples

Complete working examples in `scripts/examples/`:

### Basic Functionality
1. **Echo Bot** (`scripts/examples/basic-functionality/1-echo-bot.py`)
   - Simplest possible implementation
   - Echo responder with latency simulation

2. **Magic Assistant** (`scripts/examples/basic-functionality/2-magic-assistant.py`)
   - Custom styling with mystical theme
   - Random response selection

3. **Smart Assistant** (`scripts/examples/basic-functionality/3-smart-assistant.py`)
   - Context-aware responses
   - Keyword detection with conversation history

4. **Code Assistant** (`scripts/examples/basic-functionality/4-code-assistant.py`)
   - Syntax highlighting support
   - Multi-language code examples

### AI Integrations
1. **Pydantic AI** (`scripts/examples/ai/pydantic-ai.py`)
   - Streaming LLM responses with Pydantic AI
   - Message history management
   - Token-by-token display

## Design Principles

1. **Separation of Concerns**: Each layer has single responsibility
2. **Dependency Injection**: All dependencies explicit and injectable
3. **Protocol-Oriented**: Components communicate via protocols/interfaces
4. **Testability**: Every component testable in isolation
5. **Extensibility**: Easy to swap implementations without changing code
6. **Composition over Inheritance**: Build complex apps from simple parts

## Advanced Usage

### Multiple Responders

Route messages to different responders:

```python
class RouterResponder:
    def __init__(self):
        self.code_expert = CodeResponder()
        self.general_chat = GeneralResponder()
    
    async def __call__(self, message: str, context=None):
        if "code" in message.lower() or "```" in message:
            async for token in self.code_expert(message, context):
                yield token
        else:
            async for token in self.general_chat(message, context):
                yield token
```

### Middleware Pattern

Add logging, metrics, or transformations:

```python
class LoggingResponder:
    def __init__(self, responder):
        self.responder = responder
    
    async def __call__(self, message: str, context=None):
        logger.info(f"Request: {message}")
        async for token in self.responder(message, context):
            logger.debug(f"Token: {token}")
            yield token
        logger.info("Response complete")

app = create_default_chat_app(
    responder=LoggingResponder(MyLLMResponder())
)
```

### Custom WebSocket Logic

Access underlying FastHTML app:

```python
app = create_default_chat_app(responder=my_responder)
fasthtml_app = app.get_app()

@fasthtml_app.route("/custom")
def custom_route():
    return "Custom endpoint"

app.run(port=5001)
```

## Deployment

Pylogue uses Uvicorn under the hood:

```python
# Development
app.run(host="localhost", port=5001, reload=True)

# Production
app.run(host="0.0.0.0", port=8000, reload=False, workers=4)
```

Or use Uvicorn directly:

```python
import uvicorn
from pylogue.chatapp import create_default_chat_app

app = create_default_chat_app(responder=my_responder)
uvicorn.run(app.get_app(), host="0.0.0.0", port=8000)
```

## Documentation

Full notebook-based documentation in `nbs/`:
- `0-Card.ipynb` - Message card styling and rendering
- `1-Session.ipynb` - Session and message management
- `2-Service.ipynb` - Business logic and responders
- `3-Renderer.ipynb` - UI component rendering
- `4-ChatApp.ipynb` - Full application integration
- `5-Examples.ipynb` - Complete working examples

## Installation

```bash
pip install pylogue
```

## Requirements

- Python 3.10+
- FastHTML
- Uvicorn
- Optional: Pydantic AI, OpenAI, Anthropic, etc. for LLM integrations

## Contributing

Pylogue is fully modular. To extend:
1. Implement the relevant Protocol (SessionManager, Responder, ErrorHandler)
2. Inject your implementation
3. Test independently
4. Compose with other components

No need to modify core code!

## License

See LICENSE file for details.
